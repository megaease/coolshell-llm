{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be30e87-b232-48c4-a08c-480769fbde26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"bitsandbytes>=0.39.0\" loralib\n",
    "!pip install \"transformers>=4.31.0,<4.35.0\"\n",
    "!pip install \"datasets>=2.14.3\"\n",
    "!pip install \"accelerate>=0.21.0\"\n",
    "!pip install \"peft==0.8.1\"\n",
    "!pip install \"trl>=0.7.4\"\n",
    "!pip install \"sentencepiece\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484fca3-cdc4-4ef1-9ebb-aa06c36ab98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4616b6-db8d-4b41-a93d-a8c75e0dbae5",
   "metadata": {},
   "source": [
    "### load tokenizer\n",
    "The `AutoTokenizer.from_pretrained()` function is used to load the tokenizer that corresponds to the pre-trained model. The tokenizer is responsible for converting input text into a format that the model can understand. The `trust_remote_code=True` argument allows the execution of remote code, which can be necessary if the tokenizer includes custom (non-standard) components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e20e3-41c8-4fd1-a6f8-114eb09f4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "model_to_load = \"THUDM/chatglm3-6b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_to_load, trust_remote_code=True)\n",
    "\n",
    "prefix: List[Union[str, Dict[str, str]]] =[\n",
    "        {\"token\": \"[gMASK]\"},\n",
    "        {\"token\": \"sop\"},\n",
    "        {\"token\": \"<|user|>\"},\n",
    "        \"\\n\",\n",
    "        \"{{question}}\",\n",
    "        {\"token\": \"<|assistant|>\"}\n",
    "]\n",
    "\n",
    "eos_ids = [] if tokenizer.eos_token_id is None else [tokenizer.eos_token_id]\n",
    "bos_ids = [] if tokenizer.bos_token_id is None else [tokenizer.bos_token_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0084b6-697c-4013-8f17-3eac278197a0",
   "metadata": {},
   "source": [
    "### load dataset\n",
    "This cell code snippet is using the `load_dataset` function from the `datasets` library to load a JSON dataset from a local file named 'qa.json'. The `load_dataset` function returns a `Dataset` object.\n",
    "\n",
    "The `Dataset` object, `qa_dataset`, has its columns renamed for clarity and consistency. The 'instruction' column is renamed to 'question', the 'input' column is renamed to 'context', and the 'output' column is renamed to 'answers'. The `rename_column` method is used to perform these renamings. It takes two arguments: the current name of the column and the new name for the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822edf4-2d2f-48cf-a484-67a7a9c7fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "qa_dataset = load_dataset('json', data_files='qa.json')\n",
    "qa_dataset = qa_dataset.rename_column('instruction',  'question')\\\n",
    "        .rename_column('input', 'context')\\\n",
    "        .rename_column('output','answers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194659ec-a98e-426f-acf7-33c301183f6e",
   "metadata": {},
   "source": [
    "### processing dataset\n",
    "\n",
    "The cell code is responsible for processing a dataset for pretraining a language model. Here's a breakdown of the main parts:\n",
    "\n",
    "- create_prompt(question) -> List[int]: This function takes a question as input and returns a list of integers. It iterates over the prefix (which is not defined in the selection), and for each part of the prefix, it checks if it's a dictionary. If it is, it converts the \"token\" value to IDs using the tokenizer. If it's not a dictionary, it replaces \"{{question}}\" with the actual question and encodes it to IDs. The result is a list of token IDs that represent the prompt.\n",
    "- process_pretrain_dataset(example: \"Dataset\") -> Dict[str, List[List[int]]]: This function processes the dataset for pretraining. It takes a dataset example as input and returns a dictionary with keys \"input_ids\", \"attention_mask\", and \"labels\". It defines a generator function construct_prompt that yields context, question, and answer from the example. For each context, question, and answer, it creates a tokenized prompt and response, constructs the input IDs, labels, and attention mask, and appends them to the result dictionary.\n",
    "- print_supervised_dataset_example(example: Dict[str, List[int]]) -> None: This function prints an example from the processed dataset. It prints the input IDs, the decoded inputs, the label IDs, and the decoded labels (excluding those with a value of -100).\n",
    "- The last two lines of the selection map the process_pretrain_dataset function to the qa_dataset, removing the 'answers', 'context', and 'question' columns. It then prints the mapped dataset and an example from the \"train\" split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5e7a9-c89e-42cc-8465-2762bc57230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Any, Dict, Generator, List, Literal, Tuple, Union\n",
    "def create_prompt(question) -> List[int]:\n",
    "    result = []\n",
    "    for prefix_part in prefix:\n",
    "        if isinstance(prefix_part, dict):\n",
    "            if \"token\" in prefix_part:\n",
    "                result += [tokenizer.convert_tokens_to_ids(prefix_part[\"token\"])]\n",
    "            else:\n",
    "                result += [tokenizer.convert_tokens_to_ids(prefix_part[\"token\"])]\n",
    "        else:\n",
    "            prefix_part = prefix_part.replace(\"{{question}}\", question, 1)\n",
    "            result += tokenizer.encode(prefix_part, add_special_tokens=False)\n",
    "    return  result\n",
    "\n",
    "def process_pretrain_dataset(example: \"Dataset\") -> Dict[str, List[List[int]]]:\n",
    "    result = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    def construct_prompt(example: Dict[str, Union[str, List[str]]]) -> Generator[str, None, None]:\n",
    "        for i in range(len(example[\"question\"])):\n",
    "            context = example[\"context\"][i]\n",
    "            question = example[\"question\"][i]\n",
    "            answer = example[\"answers\"][i]\n",
    "            yield context, question, answer\n",
    "    \n",
    "    for context, question, answer in construct_prompt(example):\n",
    "        tokenized_prompt = create_prompt(question)\n",
    "        tokenized_resp = tokenizer.encode(answer, add_special_tokens=False)\n",
    "        input_ids = bos_ids  + tokenized_prompt + tokenized_resp + eos_ids\n",
    "        labels = bos_ids + [-100] * (len(tokenized_prompt)) + tokenized_resp + eos_ids\n",
    "        attention_mask = [1]*len(input_ids)\n",
    "        result[\"input_ids\"].append(input_ids)\n",
    "        result[\"attention_mask\"].append(attention_mask)\n",
    "        result[\"labels\"].append(labels)\n",
    "\n",
    "    return result\n",
    "\n",
    "def print_supervised_dataset_example(example: Dict[str, List[int]]) -> None:\n",
    "    print(\"input_ids:\\n{}\".format(example[\"input_ids\"]))\n",
    "    print(\"inputs:\\n{}\".format(tokenizer.decode(example[\"input_ids\"], skip_special_tokens=False)))\n",
    "    print(\"label_ids:\\n{}\".format(example[\"labels\"]))\n",
    "    print(\"labels:\\n{}\".format(\n",
    "        tokenizer.decode(list(filter(lambda x: x != -100, example[\"labels\"])), skip_special_tokens=False)\n",
    "    ))\n",
    "\n",
    "mapped_qa_dataset = qa_dataset.map(process_pretrain_dataset, remove_columns=['answers', 'context', 'question'], batched=True)\n",
    "print(mapped_qa_dataset)\n",
    "print_supervised_dataset_example(next(iter(mapped_qa_dataset[\"train\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8bcb0-ed11-466c-9f62-49118d5ec44a",
   "metadata": {},
   "source": [
    "### load pretrained model\n",
    "\n",
    "This cell is responsible for loading a pre-trained model and its corresponding tokenizer. \n",
    "\n",
    "The variable `model_to_load` is set to the string \"THUDM/chatglm3-6b\", which is likely the identifier of a pre-trained model stored in a model hub or a local directory.\n",
    "\n",
    "The `AutoModelForCausalLM.from_pretrained()` function is used to load the pre-trained model. This function is part of the `transformers` library and is designed to handle models that are used for causal language modeling tasks. The arguments passed to this function configure the model's behavior:\n",
    "\n",
    "- `model_to_load` specifies the model to load.\n",
    "- `torch_dtype=torch.bfloat16` sets the data type of the model's parameters to bfloat16, a floating-point format that provides better performance on some hardware.\n",
    "- `device_map='cuda:0'` specifies that the model should be loaded onto the first CUDA device, if available.\n",
    "- `trust_remote_code=True` allows the execution of remote code, which can be necessary if the model includes custom (non-standard) components.\n",
    "- `revision=\"b098244a71fbe69ce149682d9072a7629f7e908c\"` specifies a particular version of the model to load, identified by its commit hash.\n",
    "- `quantization_config=BitsAndBytesConfig(...)` sets the configuration for quantization, a technique used to reduce the memory footprint of the model. The `BitsAndBytesConfig` object is configured to use 4-bit quantization, with bfloat16 as the compute data type, and to use double quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd1b8f-832c-4ece-8edc-c84857831e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_to_load, \n",
    "                                             torch_dtype=torch.bfloat16, \n",
    "                                             device_map='cuda:0',\n",
    "                                             trust_remote_code=True,\n",
    "                                             revision=\"b098244a71fbe69ce149682d9072a7629f7e908c\",\n",
    "                                             quantization_config=BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                                               bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                                               bnb_4bit_use_double_quant=True,\n",
    "                                                               bnb_4bit_quant_type=\"nf4\")\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f6a82b-26eb-40df-9455-024478f732a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb710d-45d9-4dfe-bd3f-34ae25a85e0c",
   "metadata": {},
   "source": [
    "### set up the model\n",
    "\n",
    "This cell snippet is part of a process known as model fine-tuning, where a pre-trained model is adapted to a new, similar task.\n",
    "\n",
    "In the first loop, the code iterates over all the parameters of the model. For each parameter, it sets `requires_grad` to `False`, effectively freezing the parameter. This means that during subsequent training, the gradients will not be computed for these parameters, and thus they will remain unchanged. This is typically done when you want to keep parts of the model fixed and only train some layers (in this case, adapters will be trained later).\n",
    "\n",
    "However, if the parameter's dimension (`ndim`) is 1, the code changes the data type of the parameter's data to `torch.float32`. This is done for stability reasons, as some parameters like those in Layer Normalization layers are sensitive to precision and can cause instability in training if kept in lower precision formats.\n",
    "\n",
    "After that, the `gradient_checkpointing_enable` method is called on the model. Gradient checkpointing is a technique to reduce the memory usage when training models, at the cost of increased computation. It reduces the number of activations that need to be stored in memory.\n",
    "\n",
    "Finally, the `enable_input_require_grads` method is called on the model. This method ensures that gradients with respect to the input are computed, which is not the default behavior in PyTorch. This might be necessary for some specific training regimes or for certain types of models.\n",
    "\n",
    "The class `CastOutputToFloat` that inherits from `nn.Sequential`, a container class in the PyTorch library. The `nn.Sequential`  class is used to encapsulate a sequence of modules where the output of one module is the input to the next one.\n",
    "\n",
    "The `forward` method is overridden in the `CastOutputToFloat` class. This method is called when you pass an input to an instance of the class. The `forward` method takes an input `x`, passes it to the `forward` method of the superclass (`nn.Sequential`), and then converts the output to `torch.float32` data type using the `to` method. This is done to ensure that the output of the model is always in floating point format, which is necessary for many downstream tasks in machine learning.\n",
    "\n",
    "The last line of the code is replacing the `output_layer` of the `model.transformer` with an instance of `CastOutputToFloat`. This means that whenever the output layer of the transformer model is called, it will now use the `forward` method defined in `CastOutputToFloat`, thus ensuring that its output is always a floating point tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4a730-4bdb-4368-9077-896245315121",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False #freeze the model - train adapters later\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32)\n",
    "    \n",
    "model.gradient_checkpointing_enable() #reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.transformer.output_layer = CastOutputToFloat(model.transformer.output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f6104-a32f-40e4-949b-a616ba19ccc5",
   "metadata": {},
   "source": [
    "### print_trainable_parameters\n",
    "This Python function, `print_trainable_parameters`, takes a model as an argument and prints the number of trainable parameters in the model.\n",
    "\n",
    "The function initializes two counters, trainable_params and all_params, to zero. It then iterates over all the parameters of the model using the named_parameters() method. For each parameter, it adds the total number of elements in the parameter tensor to all_params using the numel() method.\n",
    "\n",
    "If the parameter requires gradient (i.e., it's trainable), the function also adds the number of elements in the parameter tensor to trainable_params.\n",
    "\n",
    "Finally, the function prints the number of trainable parameters, the total number of parameters, and the percentage of parameters that are trainable. This information can be useful for understanding the capacity of the model and how much of it is being trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0b752-ddd9-4c9e-b25b-3ece60cb437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the models\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_params} || trainable %: {100 * trainable_params / all_params}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd07e1-fb1d-4365-a9fe-c8b179a53de6",
   "metadata": {},
   "source": [
    "### configuring Lora configuration\n",
    "\n",
    "The cell code is configuring and applying a LoraConfig to a model, and then printing the number of trainable parameters in the model.\n",
    "\n",
    "First, a `LoraConfig` object is created with the following parameters:\n",
    "\n",
    "- `r`: This is set to 64. In the context of Lora (Layer-wise Learning Rate Adaptation), `r` is the rank of the low-rank approximation used in the Lora method.\n",
    "- `lora_alpha`: This is set to 32. `lora_alpha` is a hyperparameter in the Lora method that controls the learning rate adaptation.\n",
    "- `target_modules`: This is set to `[\"query_key_value\"]`. It specifies the modules in the model to which Lora should be applied.\n",
    "- `lora_dropout`: This is set to 0.05. It's the dropout rate used in the Lora method.\n",
    "- `bias`: This is set to \"none\". It specifies the type of bias to be used in the Lora method.\n",
    "- `task_type`: This is set to \"CAUSAL_LM\". It specifies the type of task the model is being trained for. In this case, it's a causal language modeling task.\n",
    "\n",
    "Next, the `get_peft_model` function is called with the `model` and `config` as arguments. This function is likely applying the Lora configuration to the model.\n",
    "\n",
    "Finally, the `print_trainable_parameters` function is called with the `model` as an argument. This function prints the number of trainable parameters in the model. This is useful for understanding the complexity of the model and how many parameters will be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f74a6f-56c6-4bd1-a91a-54f807a0380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e1ee5-04be-44c7-8e0a-16c6a169f18a",
   "metadata": {},
   "source": [
    "### load and print pretrained configuration\n",
    "\n",
    "This cell code snippet is used to load a pre-trained model configuration using the `AutoConfig.from_pretrained` method from the `transformers` library.\n",
    "\n",
    "The `model_to_load` variable is expected to be a string that specifies the model to be loaded. This could be a model ID from Hugging Face's model hub or a local path to a directory containing model files.\n",
    "\n",
    "The `config_kwargs` dictionary is used to pass additional arguments to the `from_pretrained` method. In this case, the `trust_remote_code` key is set to `True`, which means that the code will trust user code and data loaded from the Hugging Face model hub.\n",
    "\n",
    "After the configuration is loaded, it is printed to the console along with the `config_kwargs` dictionary for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ac91f-59f2-4a13-b50c-6f1f26ddfc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_kwargs = {\n",
    "    \"trust_remote_code\": True,\n",
    "}\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_to_load, **config_kwargs)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3406f0d4-bacc-4f3c-a99b-2e9495c6c788",
   "metadata": {},
   "source": [
    "### setting up the training configuration\n",
    "\n",
    "This cell is setting up the training configuration for a model using the `TrainingArguments` class from the `transformers` library.\n",
    "\n",
    "- `max_steps = 6000`: This sets the total number of training steps to 6000. A training step is one gradient update. In one step, the model processes `batch_size` number of examples and updates the weights once based on the gradients computed from those `batch_size` number of examples.\n",
    "\n",
    "- `logging_steps = 100`: This sets the frequency of logging steps. The training metrics will be logged every 100 steps.\n",
    "\n",
    "- `learning_rate = 5e-5`: This sets the learning rate for the optimizer. The learning rate controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
    "\n",
    "- `training_args = transformers.TrainingArguments(...)`: This creates an instance of the `TrainingArguments` class, which is used to define the training parameters. The parameters include:\n",
    "  - `per_device_train_batch_size=1`: The number of training examples utilized in one iteration per device.\n",
    "  - `gradient_accumulation_steps=1`: The number of steps to accumulate gradients before performing an optimizer step. This can be useful to handle large batches that don't fit in memory.\n",
    "  - `warmup_steps=100`: The number of steps for the warmup phase, where the learning rate increases from 0 to the initial lr set.\n",
    "  - `max_steps=max_steps`: The total number of training steps.\n",
    "  - `learning_rate=learning_rate`: The learning rate for the optimizer.\n",
    "  - `bf16=True`: Enables bfloat16 mode for training on GPUs. Bfloat16 is a compact numeric format that uses half the bits as float32 but achieves comparable model accuracy.\n",
    "  - `logging_steps=logging_steps`: The number of steps between each logging.\n",
    "  - `output_dir='outputs'`: The directory where the model predictions and checkpoints will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348230cf-f968-4c18-b175-2850a0b2282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "max_steps = 50000 # about seven hours\n",
    "logging_steps = 500\n",
    "learning_rate = 4e-5\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=100,\n",
    "        max_steps=max_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        bf16=True,\n",
    "        logging_steps=logging_steps,\n",
    "        output_dir='outputs',\n",
    "        optim='paged_adamw_8bit',    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9afbb-4ad0-403d-bfaa-46a3074e2fe1",
   "metadata": {},
   "source": [
    "### training the model\n",
    "\n",
    "This cell is responsible for the training of the model. \n",
    "\n",
    "The `transformers.Trainer` class from the `transformers` library is used to handle the training. It takes several arguments:\n",
    "\n",
    "- `model`: This is the model that you want to train. It's loaded from previous cell.\n",
    "- `train_dataset`: This is the dataset that you want to use for training. In this case, it's `mapped_qa_dataset[\"train\"]`, which is a dataset that has been preprocessed and tokenized.\n",
    "- `args`: These are the training arguments that define the training parameters such as the batch size, learning rate, etc. They are defined earlier in the cell code as `training_args`.\n",
    "- `data_collator`: This is used to batch data from the dataset. In this case, `transformers.DataCollatorForLanguageModeling` is used with `mlm` (Masked Language Modeling) set to `False`.\n",
    "\n",
    "After the `Trainer` is initialized, the model's caching mechanism is disabled by setting `model.config.use_cache` to `False`. This is done to save memory during training, but it might slow down the training process.\n",
    "\n",
    "Finally, the training is started with `trainer.train()`. This will train the model according to the parameters defined in `training_args` on the `train_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9effb2-2ee7-4ee4-96c0-4433563e3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=mapped_qa_dataset[\"train\"],\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "#model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e2905b-aaec-4776-82db-70af71d546c2",
   "metadata": {},
   "source": [
    "### saving model and state\n",
    "\n",
    "The two lines of code are used to save the state of the trainer and the model after training.\n",
    "\n",
    "The trainer.save_model() function is a method from the Trainer class in the transformers library. It saves the model's weights into a directory. By default, this directory is the one defined in the output_dir attribute of the TrainingArguments object used when initializing the Trainer.\n",
    "\n",
    "The trainer.save_state() function is also a method from the Trainer class in the transformers library. It saves the optimizer and the scheduler states to ensure that you can resume training exactly where you left off. This is particularly useful when training large models that can't be trained in one go and need to be trained in several stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754c1f2-dacc-47ab-a802-f061319ab11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d465da-5802-48b9-bf5b-3c344dc62564",
   "metadata": {},
   "source": [
    "### inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076eaabe-1b55-49b8-8d93-94ffae01c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import PeftModel\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_to_load = \"THUDM/chatglm3-6b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_to_load, \n",
    "                                             torch_dtype=torch.bfloat16, \n",
    "                                             device_map='cuda:0',\n",
    "                                             trust_remote_code=True,\n",
    "                                             revision=\"b098244a71fbe69ce149682d9072a7629f7e908c\",\n",
    "                                             quantization_config=BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                                               bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                                               bnb_4bit_use_double_quant=True,\n",
    "                                                               bnb_4bit_quant_type=\"nf4\")\n",
    "                                            )\n",
    "\n",
    "qa_model = PeftModel.from_pretrained(model, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9533b8d-64bd-4604-a2b3-3fe6ef3bf76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qa_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea14c57-2636-4a51-9c94-e0567b796138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_to_load, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043e88f-f276-4eca-8b63-60d9f5e9df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from typing import TYPE_CHECKING, Any, Dict, Generator, List, Literal, Tuple, Union\n",
    "\n",
    "prefix: List[Union[str, Dict[str, str]]] =[\n",
    "        {\"token\": \"[gMASK]\"},\n",
    "        {\"token\": \"sop\"},\n",
    "        {\"token\": \"<|user|>\"},\n",
    "        \"\\n\",\n",
    "        \"{{question}}\",\n",
    "        {\"token\": \"<|assistant|>\"}\n",
    "]\n",
    "\n",
    "def create_prompt(question) -> List[int]:\n",
    "    result = []\n",
    "    for prefix_part in prefix:\n",
    "        if isinstance(prefix_part, dict):\n",
    "            if \"token\" in prefix_part:\n",
    "                result += [tokenizer.convert_tokens_to_ids(prefix_part[\"token\"])]\n",
    "            else:\n",
    "                result += [tokenizer.convert_tokens_to_ids(prefix_part[\"token\"])]\n",
    "        else:\n",
    "            prefix_part = prefix_part.replace(\"{{question}}\", question, 1)\n",
    "            result += tokenizer.encode(prefix_part, add_special_tokens=False)\n",
    "    return  result\n",
    "\n",
    "\n",
    "def make_inference(question, refer_model):\n",
    "    batch = dict()\n",
    "    input_ids = create_prompt(question)\n",
    "    batch[\"input_ids\"] = torch.tensor([input_ids])\n",
    "    batch[\"attention_mask\"] = torch.tensor([[1] * len(input_ids)])\n",
    "    batch[\"position_ids\"] = torch.tensor([list(range(0, len(input_ids)))])\n",
    "    \n",
    "    print(\"question:\\n{}\".format(batch))\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = refer_model.generate(**batch, max_new_tokens=512)\n",
    "    \n",
    "    print(\"output{}\".format(output_tokens))\n",
    "    display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad093e-73df-47af-a795-9aec6c80f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"给立志成为架构师的程序员一些建议吧\"\n",
    "\n",
    "make_inference(question, qa_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f52a03-fa14-4df8-97e4-2358c2d8ce78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

Question: Facebook 的系统架构是怎样的？
Web 前端是由 PHP 写的。Facebook 的 HipHop 会把PHP转成 C++ 并用 g++编译，这样就可以为模板和Web逻贺业务层提供高的性能。业务逻辑以Service的形式存在，其使用Thrift。这些Service根据需求的不同由PHP，C++或Java实现（也可以用到了其它的一些语言……）用Java写的Services没有用到任何一个企业级的应用服务器，但用到了Facebook自己的定制的应用服务器。看上去好像是重新发明轮子，但是这些Services只被暴露给Thrift使用（绝大所数是这样），Tomcat太重量级了，即使是Jetty也可能太过了点，其附加值对Facebook所需要的没有意义。
持久化由MySQL, Memcached, Facebook 的 Cassandra, Hadoop 的 HBase 完成。Memcached 使用了MySQL的内存Cache。Facebook 工程师承认他们的Cassandra 使用正在减少因为他们更喜欢HBase，因为它的更简单的一致性模型，以到其MapReduce能力。离线处理使用Hadoop 和 Hive。日志，点击，feeds数据使用Scribe，把其聚合并存在 HDFS，其使用Scribe-HDFS，因而允许使用MapReduce进行扩展分析。
BigPipe 是他们的定制技术，用来加速页面显示。Varnish Cache 用作HTTP代理。他们用这个的原因是高速和有效率。 用来搞定用户上传的十亿张照片的存储，其由Haystack处理，Facebook自己开发了一个Ad-Hoc存储方案，其主要做了一些低层优化和“仅追加”写技术 .Facebook Messages 使用了自己的架构，其明显地构建在了一个动态集群的基础架构上。业务逻辑和持久化被封装在一个所谓的’Cell’。每个‘Cell’都处理一部分用户，新的‘Cell’可以因为访问热度被添加。 持久化归档使用HBase。
Facebook Messages 的搜索引擎由存储在HBase中的一个倒置索引的构建。 Facebook 搜索引擎实现细节据我所知目前是未知状态。Typeahead 搜索使用了一个定制的存储和检索逻辑。 Chat 基于一个Epoll 服务器，这个服务器由Erlang 开发，由Thrift存取。关于那些供给给上述组件的资源，下面是一些信息和数量，但是有一些是未知的：
Facebook估计有超过60,000 台服务器。他们最新的数据中心在俄勒冈州的Prineville，其基于完全自定设计的硬件那是最近才公开的 Open Compute 项目。300 TB 的数据存在 Memcached 中处理。他们的Hadoop 和 Hive 集群由3000 服务器组成，每台服务器有8个核，32GB的内存，12TB的硬盘，全部有2万4千个CPU的核，96TB内存和36PB的硬盘。每天有1000亿的点击量，500亿张照片， 3 万亿个对象被 Cache，每天130TB的日志（2010年7月的数据）
来自：酷壳网 CoolShell